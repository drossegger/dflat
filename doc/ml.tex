Classification experiments where carried out to test if the data retrieved in the benchmarks lends itself to classification. The experiments where carried out using WEKA~\cite{WEKA}, as classifier support vectore machines where used through the \lstinline$libsvm$ package available for all major operating systems. The classifier was tested using $10$-fold cross validation on the \emph{Dominating Set} and \emph{SAT} dataset obtained from the benchmarks seperatly as well as both datasets joined. The data was labeled with numbers $0-6$ for the fastest portfolio according to the \emph{user} time from the benchmarks.
The labels together with its portfolio can be seen in Table~\ref{tbl:mlLabel}. The experiments have been performed on the data with and without the label $6$, \emph{eq} which says that the runtimes were equal for all instances. For the experiments without the \emph{eq} label the instances with equal runtimes have been labelled $5$.
\begin{table}[h]
	\center
	\begin{tabular}{|r|l|}
		\hline
		Label & Portfolio\\
		\hline
		0 & not solved\footnote{The instance was not solved by any portfolio in the given time limit} \\ %TODO Fix footnote
		1 & jumpy\\
		2 & frumpy\\
		3 & crafty\\
		4 & none \\
		5 & nopre\\
		6 & eq\\
		\hline
	\end{tabular}
	\caption{Labels assigned to the corresponding portfolios}
	\label{tbl:mlLabel}
\end{table}


\par $11$ features were used in the classification experiments. The features and their origin can be seen in Table~\ref{tbl:mlFeat}, details about the extraction can be found in Section~\ref{sec:impl}. Features tagged with \emph{user} are features usually supplied by the user when executing D-FLAT, the number of edge facts is counted in the input file and features tagged $gringo$ are features of the non-ground instance. They are also independent of the instance and therefore have the same value for all instances of an encoding. 

Before the models were built, feature selection techniques were applied to get a feel which features effect the model the most.

\begin{table}
	\center
	\begin{tabular}{|l|l|l|}
		\hline
		Name & Description & Origin \\
		\hline
		\lstinline$gcomponents$ & number of components\footnote{$function symbols+ predicate symbols$} & gringo\\ %TODO Fix footnote
		\lstinline$gnontrivial$ & number of nontrivial components & gringo\\
		\lstinline$gpredicates$ & number of predicates & gringo \\
		\lstinline$gconstraints$ & number of constraints & gringo\\
		\lstinline$nbredgefacts$ & number of edge facts & instance file\\
		\lstinline$nbredgepred$ & number of edge predicates & user \\
		\lstinline$defjoin$ & $1$ if D-FLATs default join was used, $0$ otherwise & user\\
		\lstinline$normalization$ & the normalization used (non, semi, normalized) & user\\
		\lstinline$heuristic$ & the heuristic used to create the tree decomposition & user\\
		\lstinline$dw$ & the decomposition width of the tree decomposition & D-FLAT\\
		\hline
	\end{tabular}
	\caption{Features of the datasets}
	\label{tbl:mlFeat}
\end{table}

\subsection{Experiments without \emph{eq}}
\subsubsection{Dominating Set}
The distribution of the labels can be seen in Figure~\ref{fig:dsLabelsE1}. There are no $term$ labels in this dataset since it was used prior for benchmarks and instances with runtime over the time limit where not executed again.
\begin{figure}[h]
	\center
	\includegraphics[scale=0.4]{figures/domsetLabelsE1.png}
	\caption{Distribution of labels of Dominating Set\label{fig:dsLabelsE1}}
\end{figure}
\par Three feature selection methods were used on the dataset all yielding the same result that the features \lstinline$defjoin$ and \lstinline$normalization$ are the only features impacting the model significantly in both experiments. The model was therefore built on those two features as well as on all features.

\par The model on all features had $Precision$\footnote{$Precision=\frac{tp}{tp+fp}$}$=0.39$ and $Recall$\footnote{$Recall=\frac{tp}{tp+fn}$}$=0.41$, yielding $F_1-score$\footnote{$F_1=2*\frac{precision*recall}{precision+recall}$}$=0.36$.
Using only the two aforementioned features resulted in a slightly better model having $Precision=0.4$ and $Recall=0.44$, yielding $F_1-score=0.41$. The confusion matrix for both models can be seen in Table~\ref{tbl:dsCME1}. 
\begin{table}[h]
\center
	\begin{tabular}{|c|ccccc|ccccc|}
		\hline
		\multicolumn{6}{|c|}{All Attributes} &\multicolumn{5}{|c|}{normalization, defjoin}\\
		\hline &$1$&$2$&$3$&$4$&$5$&$1$&$2$&$3$&$4$&$5$\\
		 \hline$1$&$23$&$0$&$77$ &$8$ &$1$&$62$&$0$&$42$&$2$&$3$\\
		 $2$&$3$ &$0$&$10$ &$1$ &$0$&$3$ &$0$&$5$ &$1$&$5$\\
		 $3$&$26$&$0$&$106$&$5$ &$3$&$63$&$0$&$64$&$6$&$7$\\
		 $4$&$9$ &$0$&$42$ &$12$&$5$&$1$ &$0$&$48$&$3$&$16$\\
		 $5$&$14$&$0$&$27$ &$30$&$3$&$2$ &$0$&$21$&$1$&$50$\\
		 \hline
	\end{tabular}
	\caption{Confusion Matrix for Dominating Set}
	\label{tbl:dsCME1}
\end{table}
\par The model using all features overclassifies instances as $3$ (\emph{crafty}), resulting in a high recall of $77.5$ of this label but bad recall for all other labels. The precision however remains fairly balanced throughout the labels. It is interesting to see that both classifiers were not able to label an instance of $2$ (\emph{frumpy}) correctly since there are only very few instances labeled $2$ in the dataset. The classifier on only \lstinline$normalization$ and \lstinline$defjoin$ still leans to overclassifying instances as $3$ but not to the same extent as the model working on all features.
Therefore it has better precision of $0.62$ and is overall the slightly better classifier. This also shows when looking at the correctly classfied instances. The model built on all features classified $41\%$ of the instances correctly, while the model built on only $normalization$ and $defjoin$ classified $45\%$ correctly.

\subsubsection{SAT}
The distribution of the labels of the instances can be seen in Figure~\ref{fig:satLabelsE1}. This dataset contains instances which could not be solved in the specified maximum time in the benchmark, even the majority of the instances belongs to that class. 

\begin{figure}[h]
	\center
	\includegraphics[scale=0.4]{figures/satLabels.png}
	\caption{Labels of SAT\label{fig:satLabelsE1}}
\end{figure}

\par Again feature selection methods were used to determine the features with the most impact on the result. Interestingly the heuristics determined $dw$ and $nbredgefacts$ to have big impact, $normalization$ and $defjoin$ to have a little bit of impact and the other features to have very little to no impact on the prediction quality. Therefore models where built using these four features, all features  and only $dw$,$nbredgefacts$.

The model built on all features has $precision=0.32$ and $recall=0.42$, yielding $F_1=0.35$ while the model built on $nomalization$, $dw$, $nbredgefacts$ and $defjoin$ has $precision=0.31$ and $recall=0.41$, yielding $F_1 =0.35$. The classifier built of only $dw$ and $nbredgefacts$ has $precision=0.32$ and recall $F_1=0.42$. According to the $F_1$ score the model on all features and the model built on only $dw$ and $nbredgefacts$ perform equally, while the model built on the four features predicted best by the feature selection heuristics performs sligthly worse. Looking at the correctly classified instances this effect can be seen too. The models on all features and on $dw$ and $nbredgefacts$ classified $42$ ($42\%$) correctly, while the model on four features classified $41$ ($41\%$) instances correctly. The confusion matrix for the models can be seen in Table~\ref{tbl:satCME1}.

\begin{table}[h]
	\center
	\begin{tabular}{|c|cccccc|cccccc|cccccc|}
		\hline\multicolumn{7}{|c|}{All Attributes} &\multicolumn{6}{|c|}{$4$ attributes}&\multicolumn{6}{|c|}{dw, nbredgefacts}\\
		\hline &$0$&$1$&$2$&$3$&$4$&$5$&$0$&$1$&$2$&$3$&$4$&$5$&$0$&$1$&$2$&$3$&$4$&$5$\\
		\hline $0$ & $4$ & $0$ & $0$ & $2$ & $1$ & $12$& $4$ & $0$ & $0$ & $2$ & $2$ & $11$& $4$ & $0$ & $0$ & $3$ & $2$ & $10$\\
					 $1$ & $1$ & $0$ & $0$ & $0$ & $0$ & $3$ & $1$ & $0$ & $0$ & $0$ & $0$ & $3$ & $1$ & $0$ & $0$ & $0$ & $0$ & $3$\\
					 $2$ & $1$ & $0$ & $0$ & $0$ & $1$ & $2$ & $1$ & $0$ & $0$ & $0$ & $1$ & $2$ & $1$ & $0$ & $0$ & $0$ & $1$ & $2$\\
					 $3$ & $3$ & $0$ & $0$ & $0$ & $2$ & $5$ & $3$ & $0$ & $0$ & $0$ & $2$ & $5$ & $3$ & $0$ & $0$ & $0$ & $2$ & $5$\\
					 $4$ & $4$ & $0$ & $0$ & $0$ & $3$ & $12$& $4$ & $0$ & $0$ & $0$ & $3$ & $12$& $4$ & $0$ & $0$ & $1$ & $3$ & $11$\\
					 $5$ & $4$ & $0$ & $0$ & $0$ & $5$ & $35$& $5$ & $0$ & $0$ & $0$ & $5$ & $34$& $5$ & $0$ & $0$ & $0$ & $4$ & $35$\\
		\hline
	\end{tabular}
	\caption{Confusion Matrix for SAT}
	\label{tbl:satCME1}
\end{table}
\subsubsection{All Instances}
The experiments in this section are the most meaningful since they work with the data from both benchmarks. The input data was obtained by combining the data from both benchmarks.

Again feature selection heuristics were used on the dataset, selecting a subset of $4$ features, $gcomponents$, $gnontrivial$, $nbredgefacts$ and $dw$ as the most impactful features. In contrast to the other experiments all features but $heuristic$ had some impact on the result. Therefore models were built on the subset as well as on all features.

The model built on all features has $precision=0.38$ and $recall=0.42$, yielding $F_1=0.38$, while the model built on the subset performed worse, having $precision=0.3$ and $recall=0.35$, yielding $F_1=0.31$. The model build on all features classified $42\%$ of the instances correctly, while the model built on the subset classified $35\%$ correctly. This strengthens the observation made during feature selection that all features but $heuristic$ impact the result. Another experiment was made using all features but $heuristic$, producing exactly the same results as the experiment on all features, strengthening the observation even further. The confusion matrix of both models can be seen in Table~\ref{tbl:cmbCM}.

\begin{table}[h]
	\center
	\begin{tabular}{|c|cccccc|cccccc|}
		\hline\multicolumn{7}{|c|}{All Attributes} &\multicolumn{6}{|c|}{$4$ attrib}\\
		\hline &$0$&$1$&$2$&$3$&$4$&$5$&$0$&$1$&$2$&$3$&$4$&$5$\\
		\hline $0$ & $4$ & $0$ & $0$ & $3$ & $3$ & $9$ & $4$ & $0$ & $0$ & $3$ & $3$ & $9$ \\
					 $1$ & $1$ & $24$& $0$ & $75$& $3$ & $11$& $1$ & $16$& $0$ & $65$& $10$& $22$\\
           $2$ & $1$ & $3$ & $0$ & $8$ & $2$ & $4$ & $1$ & $3$ & $0$ & $9$ & $2$ & $3$\\
				   $3$ & $1$ & $21$& $0$ &$105$& $8$ & $15$& $1$ & $20$& $0$ &$104$& $9$ & $16$\\
				   $4$ & $2$ & $9$ & $0$ & $37$& $11$& $28$& $2$ & $14$& $0$ & $43$& $5$ & $23$\\
				   $5$ & $3$ & $12$& $0$ & $25$& $10$& $67$& $3$ & $12$& $0$ & $41$& $13$& $48$\\
		\hline
	\end{tabular}
	\caption{Confusion Matrix for all instances}
	\label{tbl:cmbCM}
\end{table}

\subsection{Experiments with $eq$}
For $Sat$ $39$ instances and for $Dominating Set$ $29$ instances wer labelled $eq$. We found it interesting to see how labeling instances where all portfolios have the same runtimes with one of the portfolios affects the model. Therefore the same experiments as in the above section where done, labeling instances with equal runtimes with a new label $eq$ instead of labeling them $nopre$. We expected the models to be better using this technique since we thought that labeling all the instances labeled $eq$ with $nopre$ might bias the model.

Surprisingly the models built with label $eq$ performed much worse than the ones without it, not exceeding $precision$ of $0.3$ and $recall$ of $0.35$, and unable to correctly classify most instances. Not more than $35\%$ of the instances were classified correctly in all experiments.

\subsection{Conclusion}
While the model for the combined instances does not seem to good considering it only classified $42\%$ of the instances correctly, one should not be disillusioned by it. For most instances the difference in runtimes of the portfolios was only marginally. Therefore classifying an instance wrong wont affect the runtime too much in most cases. 
Furthermore the model was built on only $505$ instances, it is expected that the quality of the model increases with the size of the learning base.

It would be interesting to see if the models classify correctly on instances where one of the portfolio was a lot faster than the others but to test this bigger test sets would be needed. Also a classifier built on only those instances could yield interesting results but to get enough data extensive benchmarking would be needed which would break the scope of this project.

